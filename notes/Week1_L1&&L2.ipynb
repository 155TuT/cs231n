{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ7qjFnUcbvTQp/wO4/jZL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/155TuT/cs231n/blob/main/notes/Week1_L1%26%26L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本周学习了cs231n的前2(.5?)节课.在此之前我对神经网络的印象一直停留在堆叠算法与资源后，模型就黑魔法一样的学会了复杂任务这种非常朴素的直觉上。但显然，经过这两节课对神经网络的底层逻辑和工作原理的了解后，这本黑魔法书也终于开始向我展示其震撼了。\n",
        "\n",
        "## 学习内容\n",
        "\n",
        "1. 图像分类\n",
        "\n",
        "cs231n开篇用图像分类问题作为引子：给定一张图片，模型要判断它属于哪一类，比如猫、狗还是汽车。这是计算机视觉最基础的任务，也引出了神经网络这种强大的方法：图像可以被看作一个高维的向量，比如一张32x32的RGB图片本质上是一个**长为3072（32x32x3）**的向量。神经网络就是要从这个长向量里**自动**学习到有用的模式，而无需手工设计特征。以前的传统方法是：\n",
        "\n",
        "- 曼哈顿距离(L1)到欧氏距离（L2）的k-最近邻分类器(K-Nearest Leighbors)\n",
        "\n",
        "- 手工提取特征（如SIFT、HOG）并用SVM/KNN做分类。\n",
        "\n",
        "而神经网络彻底改变了这个范式：端到端学习。让模型直接从像素学习特征，省去人工干预。听到这里我第一次意识到，“特征设计”这个在传统算法中被认为是核心的问题，在深度学习里完全被交给了网络本身。\n",
        "\n",
        "2. 神经网络结构\n",
        "\n",
        "老师用一个简化的全连接网络（Multi-Layer Perceptron, MLP）来说明前向传播：\n",
        "\n",
        "- 输入层：接收像素数据\n",
        "\n",
        "- 隐藏层：进行线性变换+激活函数（ReLU、Sigmoid等）\n",
        "\n",
        "- 输出层：经过Softmax归一化后，输出概率分布\n",
        "\n",
        "印象最深的是激活函数：如果每层都是线性变换，堆叠再多层整体仍是线性函数，无法拟合复杂关系。此时我曾在知乎上看到的如雷贯耳的[ReLU](\n",
        "https://www.zhihu.com/question/358255792/answer/1633475826)的引入打破了这一限制，让网络具备了表达非线性决策边界的能力。\n",
        "\n",
        "3. 损失函数与训练目标\n",
        "\n",
        "训练的目标是让模型输出的类别概率尽量接近真实标签。这里用到：\n",
        "\n",
        "- Softmax：把输出向量归一化为概率分布\n",
        "\n",
        "- 交叉熵损失（Cross-Entropy Loss）：衡量预测分布和真实分布的差异\n",
        "\n",
        "老师强调，优化模型并不是直接让准确率变高，而是最小化损失函数。也就是说，最小化损失的过程会自然提高预测的准确性。\n",
        "\n",
        "## 个人理解和感悟\n",
        "在本周的学习里，我最大的收获是：\n",
        "\n",
        "- 神经网络的黑魔法\n",
        "\n",
        "    本质是在用大量参数拟合一个复杂的函数，从输入到输出建立一个映射。成为见习炼丹师是很酷的。\n",
        "\n",
        "- 链式法则\n",
        "\n",
        "    虽然反向传播还没正式展开，但我已经感受到“梯度传播”的核心思路：通过链式法则计算每个参数对损失的影响并反向更新参数。\n",
        "\n",
        "- 特征提取\n",
        "\n",
        "    以前做算法总想写“规则”去提取特征，而在神经网络里，这些都变成了“学习”后调整的权重。\n",
        "\n",
        "## 遇到的问题\n",
        "\n",
        "学习过程中也有不少挑战：\n",
        "\n",
        "微积分与线代：在理解损失函数偏导和链式法则时略微吃力，需要多看几遍。\n",
        "\n",
        "抽象理解与代码实现：理论上明白了前向传播流程，但用numpy自己写一遍，还是会有很多细节卡住。~~（思绪重回那年学线段树）~~\n",
        "\n",
        "## 小结\n",
        "本周的学习只是开始，让我对神经网络有了第一层认知：一个通过反向传播优化参数、自动学习特征、端到端处理问题的**函数近似器**。\n",
        "\n",
        "虽然还在初步理解框架的阶段，但已经开始期待后续学习CNN层、经典网络结构（AlexNet、ResNet）的部分。希望在补好数学和代码能力后，能尽快自己实现网络。\n",
        "\n",
        "写这个的时候其实挺兴奋的，可能漏掉一些学习细节，后续会再在用到的时候补充TuT"
      ],
      "metadata": {
        "id": "eraeSAum0h5Y"
      }
    }
  ]
}