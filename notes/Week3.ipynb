{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684d478c",
   "metadata": {},
   "source": [
    "# cs231n 第三周学习简报"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ce11f",
   "metadata": {},
   "source": [
    "\n",
    "本周网课进度推进至 CNN 部分，包括卷积层、池化层的前向与反传机制；assignment2 完成至 Q3，在完稿前一日终于实现了CNN。 ~~写神经网络不难，难的是写得通。~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44a8bd",
   "metadata": {},
   "source": [
    "## 学习内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5892407",
   "metadata": {},
   "source": [
    "CNN 模块（视频部分）\n",
    "\n",
    "- 尽量理解和推导了卷积操作的 **数学定义** 与 **参数共享机制**；\n",
    "- 掌握了前向传播中 **卷积层、ReLU、池化层** 的组合方式；\n",
    "- 学习了反向传播的 **局部梯度计算**，包括卷积核和池化位置的梯度更新逻辑；\n",
    "- 注意到 CNN 的 **训练稳定性** 对初始化策略、正则项与优化器选择较为敏感，开始逐渐理解“调参”一词的复杂含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd729c",
   "metadata": {},
   "source": [
    "Assignment2 - Q1 ~ Q3\n",
    "\n",
    "- Q1：构建了支持 BatchNorm、Dropout 的全连接神经网络框架\n",
    "- Q2：实现了多种优化器（包含动量的 SGD、RMSprop、Adam），并在固定架构下测试了收敛性能\n",
    "- Q3：搭建了简易 CNN，完成 `Conv-ReLU-Pool` 的正反向传播实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eef510",
   "metadata": {},
   "source": [
    "调试过程中主要耗时在：\n",
    "\n",
    "- 维度管理：在多通道输入与卷积核堆叠下，`im2col` 与 `col2im` 的转置逻辑极易错位；\n",
    "- backward 实现：池化层反传依赖于前向最大值索引，书写较为繁琐；\n",
    "- 数值稳定性：Softmax 层在小批量训练下出现 NaN，最终通过 clip 或加 epsilon 处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9824fe4",
   "metadata": {},
   "source": [
    "## 遇到的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016b7a3",
   "metadata": {},
   "source": [
    "- 数学推导 -- 反向传播的链式推导，虽然形式上已能写出，但实际对中间张量意义的掌握仍不扎实。尤其在 CNN 中，卷积核梯度的来源一度混淆，对 `stride` 和 `padding` 的梯度贡献不够直观。\n",
    "\n",
    "- 编程思维转换 -- 以为 Python 语言较为简单，之前忽视了对其的深入研究，但在 numpy 为主的数值编程环境下，仍存在cpp式思维“惯性”。维度变化、张量广播等操作，仍然停留在“试出来”阶段，缺乏系统的理解，遑论后续的pytorch。\n",
    "\n",
    "- 学习路径 -- 课程重心偏向底层机制与从零实现，但本人习惯从已有案例入手自顶向下拆解。这种路径差异导致进度虽同步，理解却滞后，时常出现“写得出来但不知道自己在写什么”的错位感，出错时很难快速定位错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6d642",
   "metadata": {},
   "source": [
    "## 下周计划"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a572636",
   "metadata": {},
   "source": [
    "- 补全 assignment2 剩余部分，计划完成 Q4（BatchNorm 的扩展）与 Q5（PyTorch 对照实验）；\n",
    "- 复习 CNN 的数学推导，尤其是卷积与池化的梯度回传公式，整理为结构化笔记；\n",
    "- 阅读优秀同学的代码实现，尝试理清更合理的结构组织方式与 debug 思路；\n",
    "- 小规模使用 PyTorch 重写 CNN 架构，逐步建立从“从零实现”到“框架实用”的连接桥梁。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc29cf",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e1ec2",
   "metadata": {},
   "source": [
    "对CNN  debug后感觉所谓的“手写神经网络”就是在解剖自己对线性代数与函数链条等数学推导的信任。每一层的梯度都要再做一次确认，确认自己理解了什么遗漏了什么。确实有些吃力，感觉需要一些时间来熟悉一下python，更需要对数学推导多训练一些。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
